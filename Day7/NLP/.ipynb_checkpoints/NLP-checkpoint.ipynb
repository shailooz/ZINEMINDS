{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNcLrZNyQpSF"
   },
   "source": [
    "#Text Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOBMo6c3Xs1K"
   },
   "source": [
    "##Dealing with file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BksyUP3VXyTJ"
   },
   "outputs": [],
   "source": [
    "myfile = open('test.txt')\n",
    "\n",
    "content = myfile.read()\n",
    "\n",
    "print(content)\n",
    "\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn6nK9iTZqQn"
   },
   "outputs": [],
   "source": [
    "myfile = open('test.txt')\n",
    "\n",
    "content = myfile.readlines()\n",
    "\n",
    "print(content)\n",
    "\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Fwij7GnblPd"
   },
   "outputs": [],
   "source": [
    "with open('test.txt','r') as txt:\n",
    "    first_line = txt.readlines()[0]\n",
    "\n",
    "print(first_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9vWWt8VaITT"
   },
   "outputs": [],
   "source": [
    "my_file = open('outtest.txt','w+')\n",
    "\n",
    "my_file.write('This is a new first line')\n",
    "\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf4RR7hEakIq"
   },
   "outputs": [],
   "source": [
    "# prompt: create a python code to append content in above file\n",
    "\n",
    "my_file = open('outtest.txt','a')\n",
    "\n",
    "my_file.write('\\nThis is a new second line')\n",
    "\n",
    "my_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2n1Cq09avuG"
   },
   "source": [
    "##Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfHnka3jazKL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Search for a pattern in a string\n",
    "pattern = \"Hello\"\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "\n",
    "if match:\n",
    "  print(match.group())\n",
    "else:\n",
    "  print(\"Pattern not found\")\n",
    "\n",
    "# Find all occurrences of a pattern in a string\n",
    "pattern = \"Hello\"\n",
    "text = \"Hello, world! Hello, again!\"\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "print(matches)\n",
    "\n",
    "# Check if a pattern is included in a string\n",
    "pattern = \"Hello\"\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "if re.search(pattern, text):\n",
    "  print(\"Pattern is included\")\n",
    "else:\n",
    "  print(\"Pattern is not included\")\n",
    "\n",
    "# Complex regular expressions\n",
    "pattern = \"(?<=Hello).*(?=world!)\"\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn1q6fg0XcWq"
   },
   "source": [
    "# Natural Language Processing With Python's NLTK Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSBl1muAQpSP"
   },
   "source": [
    "### Getting Started With Python’s NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKwvt-WXQpSU"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na0mBMzaQpSX"
   },
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2XOqgzIQpSY"
   },
   "outputs": [],
   "source": [
    "example_string = \"\"\"Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "And the first lesson of all was the basic trust that he could learn.\n",
    "It's shocking to find how many people do not believe they can learn,\n",
    "and how many more believe learning to be difficult.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL2qYiUzQpSZ"
   },
   "source": [
    "#### Tokenizing by Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zm6TekKiQpSZ"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences=sent_tokenize(example_string)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuKFnsr-Tpbd"
   },
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "  print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbwZhuD1QpSa"
   },
   "source": [
    "#### Tokenizing by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQvxg_rfQpSb"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words=word_tokenize(example_string)\n",
    "print(words)\n",
    "for w in words:\n",
    "  print(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18Vg8Ed7U9Hx"
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "print(sentences)\n",
    "for s in sentences:\n",
    "  data.append(word_tokenize(s))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwW_RSj2QpSb"
   },
   "source": [
    "### Filtering Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-GCBoviQpSc"
   },
   "source": [
    "Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKMglYMMQpSc"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6MnI5GrQpSd"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2Oi0Ka_XBXb"
   },
   "outputs": [],
   "source": [
    "#special character removal\n",
    "#['Sir', 'protest', 'merry', 'man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bz_VSLEqQpSd"
   },
   "outputs": [],
   "source": [
    "worf_quote = \"Sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh6FiBRUQpSe"
   },
   "outputs": [],
   "source": [
    "words_in_quote = word_tokenize(worf_quote)\n",
    "print(words_in_quote)\n",
    "len(words_in_quote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKyVNbQcQpSe"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsCNNjvDQpSf"
   },
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXgCUwtPYcwL"
   },
   "outputs": [],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc-FM-e8QpSf"
   },
   "outputs": [],
   "source": [
    "filtered_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOeXA4UPuXbm"
   },
   "outputs": [],
   "source": [
    "print(words_in_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_34RKJ9QpSf"
   },
   "outputs": [],
   "source": [
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K53jWjlLQpSg"
   },
   "outputs": [],
   "source": [
    "filtered_list = [\n",
    "    word for word in words_in_quote if word.casefold() not in stop_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ-ZUAe3QpSg"
   },
   "outputs": [],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN25q9B0QpSg"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU6HZZbcYAIW"
   },
   "source": [
    "When we have many variations of a same word.eg: the root word is dance and variations are dancing, dance, danced. Stemming algorithm works by cutting the suffix from the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljE8OFaOQpSh"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# PorterStemmer is an algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohtd-mFrQpSh"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()  #steamer is an abject of porterstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4H6T6X0hQpSh"
   },
   "outputs": [],
   "source": [
    "# string_for_stemming = \"\"\"\n",
    "# The crew of the USS Discovery discovered many discoveries.\n",
    "# Discovering is what explorers do.\"\"\"\n",
    "string_for_stemming =\"The friends of DeSoto love scarves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTB62OkcQpSi"
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVosWBQZQpSi"
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sen6ITABQpSi"
   },
   "outputs": [],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR40S23oQpSj"
   },
   "outputs": [],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_inioMpVQpSm"
   },
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwLMUPsAdB_6"
   },
   "source": [
    "It switches any kind of a word to its base root form(lemma). Lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.\n",
    "\n",
    "**Why Lemmatizing is better than stemming?**\n",
    "\n",
    "Stem may not be an actual word whereas, lemma is an actual language word.\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5UPIZGWQpSm"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jmh-QcmAQpSm"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QXYleo7QpSm"
   },
   "outputs": [],
   "source": [
    " nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbqMxvQHQpSm"
   },
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')\n",
    "lemmatizer.lemmatize(\"scarves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-owJR78gQpSn"
   },
   "outputs": [],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQTZcClWQpSn"
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR_cH2HOQpSn"
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhjFFCHXQpSn"
   },
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JleqXUy3QpSo"
   },
   "outputs": [],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMhslkCLQpSo"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fN5BiVb3QpSo"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LBDmtWRfMD-"
   },
   "source": [
    "##Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIpGzbqLfQpM"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oppuyihkf6rI"
   },
   "outputs": [],
   "source": [
    "#vocabulary matching\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
    "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
    "\n",
    "matcher.add('SolarPower',[pattern1,pattern2,pattern3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-RJgnq1g6Gh"
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity.')\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJUvCLRAh6_C"
   },
   "source": [
    "## Part of Speech Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PppzLgx7hAiN"
   },
   "outputs": [],
   "source": [
    "# prompt: generate code for demonstrating pos tagginh options in spacy and visualizing it\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get the text\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the part-of-speech tags\n",
    "tags = [token.tag_ for token in doc]\n",
    "\n",
    "# Print the tags\n",
    "print(tags)\n",
    "\n",
    "# Visualize the part-of-speech tags\n",
    "displacy.render(doc, style='dep',jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adAaQHiVjryp"
   },
   "source": [
    "##Named Entity Recogntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggiDJKJDkBp7"
   },
   "outputs": [],
   "source": [
    "# prompt: demo the code for NER in detail and its visualization   on a large paragraph\n",
    "\n",
    "text = \"President Barack Obama gave a speech at the White House.\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I9xOilfQpSp"
   },
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvTZVbPhQpSp"
   },
   "source": [
    "N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETsVty1AQpSp"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C48eNyfkQpSp"
   },
   "outputs": [],
   "source": [
    "example_string = \"\"\"Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "And the first lesson of all was the basic trust that he could learn.\n",
    "It's shocking to find how many people do not believe they can learn,\n",
    "and how many more believe learning to be difficult.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10S9QNBoQpSq"
   },
   "outputs": [],
   "source": [
    "TextBlob(example_string).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQA_ZhjoQpSo"
   },
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blkoSPXTQpSq"
   },
   "source": [
    "## Bag of Words(BOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDb6csiGQpSq"
   },
   "source": [
    "Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pB227f0QpSq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uV_0SPUuQpSq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjUpwR6PQpSr"
   },
   "outputs": [],
   "source": [
    "text = [\"They love NLP\",\n",
    "        \"NLP is future\",\n",
    "        \"They will learn in two months\"]\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM4eOQLTQpSr"
   },
   "outputs": [],
   "source": [
    "text2 = ['They love NLP but can not learn in two months']\n",
    "count_array=vectorizer.transform(text2).toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzTuUb6xQpSr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = [\"food was not bad\",\"I am not feeling bad\"]\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2))\n",
    "count_matrix = vectorizer.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHXSiSx3QpSs"
   },
   "source": [
    "## Term Frequency – Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wr4zM6rdQpSs"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfTqqA6OQpSs"
   },
   "outputs": [],
   "source": [
    "text = [\"i love the NLP\",\n",
    "        \"NLP is the future\",\n",
    "        \"i will learn the NLP\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "count_array = matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37G0UK4cQpSt"
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm29MEd7QpSt"
   },
   "source": [
    "Word Embedding is the representation of text in the form of vectors. The underlying idea here is that similar words will have a minimum distance between their vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMJq6doINaNW"
   },
   "outputs": [],
   "source": [
    "X=[d.split() for d in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZujy5UZOPXf"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USZBhD3IQpSt"
   },
   "outputs": [],
   "source": [
    "#convert text into the word2vec format.\n",
    "import gensim\n",
    "w2vecmodel=gensim.models.Word2Vec(sentences=X,vector_size=2,window=4,min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj7X5ZV3QpSu"
   },
   "source": [
    "Now, we can load the above word2vec file as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuHBpap_QpSu"
   },
   "outputs": [],
   "source": [
    "print(w2vecmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pbKY0v6QpSu"
   },
   "outputs": [],
   "source": [
    "words = w2vecmodel.wv.index_to_key\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXBa49-rJM3h"
   },
   "outputs": [],
   "source": [
    "words = w2vecmodel.wv.key_to_index\n",
    "print(words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXm4bkVQQpSu"
   },
   "outputs": [],
   "source": [
    "print(words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pknB4PAUQpSv"
   },
   "outputs": [],
   "source": [
    "print(w2vecmodel.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZH8Pb2TkP_Pw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y = w2vecmodel.wv.vectors[:,0]\n",
    "x = w2vecmodel.wv.vectors[:,1]\n",
    "labels = words.keys()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "for i, txt in enumerate(labels):\n",
    "    ax.annotate(txt, (x[i], y[i]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
